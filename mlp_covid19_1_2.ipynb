{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_covid19-1-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmuller/mlp-covid19/blob/main/mlp_covid19_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfMLc5mwTWTo"
      },
      "source": [
        "# Part 1 - Loading and Cleaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqd2YGqGTWTo"
      },
      "source": [
        "## changing the indent\n",
        "Enter the snippet below into the browser's javascript console\n",
        "on chrome get to that from \"...\"->more tools->developer tools->console\n",
        "```\n",
        "var cell = Jupyter.notebook.get_selected_cell();\n",
        "var config = cell.config;\n",
        "var patch = {\n",
        "      CodeCell:{\n",
        "        cm_config:{indentUnit:2}\n",
        "      }\n",
        "    }\n",
        "config.update(patch)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNIUVntFYDWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f311096-8690-4b14-9ffb-d8b4374aba6e"
      },
      "source": [
        "# need to intall this every time on colab, not sure why but I do it\n",
        "import importlib\n",
        "import warnings\n",
        "import pdb\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "def warning_on_one_line(message, category, filename, lineno, file=None, line=None):\n",
        "    return 'line:%s  cat:<%s>  msg:%s\\n'% (lineno, category.__name__, message)  \n",
        "\n",
        "warnings.formatwarning = warning_on_one_line\n",
        "\n",
        "def global_imports(imports=None, verbosity=0):\n",
        "  fails = []\n",
        "  if not imports:\n",
        "    return\n",
        "  if not isinstance(imports, list):\n",
        "    msg = \"imports shoule be a list with elements like\\m\"\n",
        "    msg += \"'foo' or 'foo as bar'\"\n",
        "    warnings.warn(msg)\n",
        "    return\n",
        "  #pdb.set_trace()\n",
        "  for line in imports:\n",
        "    parts = line.split()\n",
        "    if len(parts) not in [1,3] or \\\n",
        "          (len(parts) == 3 and parts[1] != 'as'):\n",
        "      msg = \"<{0}> not a valid import line\\n\"\n",
        "      msg += \"use 'import foo' or 'import foo as bar'\"\n",
        "      warnings.warn(msg)\n",
        "      continue\n",
        "    module_name = parts[0]\n",
        "    asname = parts[0]\n",
        "    if len(parts) == 3:\n",
        "      asname = parts[2]\n",
        "    try:\n",
        "      module = importlib.import_module(module_name)\n",
        "      globals()[asname] = module\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(\"{0} not installed\\Trying to install it\".format(module_name))\n",
        "      try:\n",
        "        !pip install $module_name\n",
        "        module = importlib.import_module(module_name)  \n",
        "        globals()[asname] = module        \n",
        "      except Exception as e2:\n",
        "        print(e2)\n",
        "        fails.append(module_name)\n",
        "  if fails:\n",
        "    msg = \"Not able to import or install\\n{0}\".format(fails)\n",
        "  else:\n",
        "    msg = \"Success. imported {0}\".format(imports)\n",
        "  if verbosity > 0:\n",
        "    print(msg)\n",
        "  return fails\n",
        "    \n",
        "def is_defined(varnames=None, verbosity=0):\n",
        "  if isinstance(varnames, str):\n",
        "    varnames = [varnames]\n",
        "  for v in varnames:\n",
        "    try:\n",
        "      eval(v)\n",
        "      if verbosity > 0:\n",
        "        print(\"'{0}'' is defined\")\n",
        "    except Exception as e:\n",
        "      warnings.warn(\"trying to eval '{0}'\".format(v))\n",
        "      print(e)\n",
        "\n",
        "imports = [ 'fsspec',\n",
        "           'nltk',\n",
        "            'pandas as pd',\n",
        "            'numpy as np']\n",
        "varnames = ['fsspec', 'pd']\n",
        "global_imports(imports, verbosity=1)\n",
        "# check if the imports worked\n",
        "is_defined([x.split()[-1] for x in imports])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success. imported ['fsspec', 'nltk', 'pandas as pd', 'numpy as np']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpi84TZlHZM4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import unicodedata\n",
        "import warnings\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import glob\n",
        "import fsspec\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import re\n",
        "# Not using torch for this initial part\n",
        "if False:\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  from torch import optim\n",
        "  import torch.nn.functional as F\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CfosqNUxpsT",
        "outputId": "eda2d7d0-8626-414e-cd63-bfaa02c5c3f0"
      },
      "source": [
        "# code to show what versions of modules I'm using\n",
        "import inspect\n",
        "mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items()))\n",
        "vi = sys.version_info\n",
        "print(\"version {0}.{1}.{2} of Python\".format(vi.major, vi.minor, vi.micro))\n",
        "for name, mod in mlist:\n",
        "    mname = name\n",
        "    if name.startswith(\"__\"):\n",
        "      print(name, mod)\n",
        "      continue\n",
        "    if hasattr(mod, \"__version__\"):\n",
        "        mname = name\n",
        "        if hasattr(mod, \"__path__\"):\n",
        "            mname = os.path.split(mod.__path__[0])[1]\n",
        "        print(\"version {1} of {0} as {2} \".format(mname, name, mod.__version__))\n",
        "    elif hasattr(mod, \"__file__\") and \"site-packages\" in mod.__file__:\n",
        "        print(\"No __version__ for {0} as {1}\".format(mname, name))\n",
        "del mod\n",
        "del name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "version 3.6.9 of Python\n",
            "__builtin__ <module 'builtins' (built-in)>\n",
            "__builtins__ <module 'builtins' (built-in)>\n",
            "version fsspec of fsspec as 0.8.4 \n",
            "version nltk of nltk as 3.2.5 \n",
            "version pd of pandas as 1.1.4 \n",
            "version np of numpy as 1.18.5 \n",
            "version re of re as 2.2.1 \n",
            "version json of json as 2.0.9 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbOfQW87TWTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a6f763-2b1c-41a3-a5fc-4485a50a0f9c"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  # to mount google drive for the input files\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive', force_remount=False)  \n",
        "  dpath = \"/content/gdrive/My Drive/data/mlp_covid19/16119_db21c91a1ab47385bb13773ed8238c31\"  \n",
        "else:\n",
        "  print('Not running on CoLab')\n",
        "  dpath = \"./data/16119_db21c91a1ab47385bb13773ed8238c31\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqcsZjItfGB3",
        "outputId": "5a88cf31-4bb5-48ed-f8f5-16c4675a4282"
      },
      "source": [
        "# make a list of all the file paths in the data directory\n",
        "jfiles = os.listdir(dpath)\n",
        "jpaths = [os.path.join(dpath, jf) for jf in jfiles]\n",
        "print(\"\\n\".join(sorted(jfiles[:4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16119_webhose_2020_03_db21c91a1ab47385bb13773ed8238c31_0000005.json\n",
            "16119_webhose_2020_03_db21c91a1ab47385bb13773ed8238c31_0000015.json\n",
            "16119_webhose_2020_03_db21c91a1ab47385bb13773ed8238c31_0000016.json\n",
            "16119_webhose_2020_03_db21c91a1ab47385bb13773ed8238c31_0000023.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzVXqfZ1fcJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1801b2-53eb-4d07-ebbb-9649dd78fe29"
      },
      "source": [
        "# only using 2 files\n",
        "# 16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json \n",
        "# 16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json.\n",
        "\n",
        "files_list = [\"16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json\",\n",
        "      \"16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json\"]\n",
        "#check that these are indeed in the directory\n",
        "all_files = set(os.listdir(dpath))\n",
        "for f in files_list:\n",
        "    if f not in all_files:\n",
        "        warnings.warn(\"{0} not found\".format(f))\n",
        "        continue\n",
        "    fpath = os.path.join(dpath, f)\n",
        "    if not os.path.isfile(fpath):\n",
        "        warnings.warn(\"{0} found but not a file\".format(f))\n",
        "    fbytes = os.path.getsize(fpath)\n",
        "    print(\"{0} {1}  MB  {2} GB\".format(f, np.round(fbytes/10**6, 2),\n",
        "                                       np.round(fbytes/10**9, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json 0.56  MB  0.001 GB\n",
            "16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json 524.38  MB  0.524 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIy8kOgDfU_R",
        "outputId": "db4230ea-716e-4dcb-c321-f8de47258ef8"
      },
      "source": [
        "# try reading the data in the first file using the json module\n",
        "# raised error\n",
        "f1 = files_list[0]\n",
        "p1 = os.path.join(dpath, f1)\n",
        "encoding = 'utf-8'\n",
        "try:\n",
        "  with open(p1, encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  # sys.exc_info returns (type, value, traceback)\n",
        "  extype, exval, tb = sys.exc_info()\n",
        "  warnings.warn(str(extype))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extra data: line 2 column 1 (char 8133)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "line:13  cat:<UserWarning>  msg:<class 'json.decoder.JSONDecodeError'>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWvvDcg2ki25",
        "outputId": "826eed5c-83a1-44a8-f698-002bdd01b2e2"
      },
      "source": [
        "# try reading the data with pandas\n",
        "# raised error\n",
        "f1 = files_list[0]\n",
        "encoding = 'utf-8'\n",
        "p1 = os.path.join(dpath, f1)\n",
        "try:\n",
        "  data = pd.read_json(p1, encoding=encoding)\n",
        "except Exception as e:\n",
        "  print(e)   \n",
        "  # sys.exc_info returns (type, value, traceback)\n",
        "  extype, exval, tb = sys.exc_info()\n",
        "  warnings.warn(str(extype)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trailing data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "line:12  cat:<UserWarning>  msg:<class 'ValueError'>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaivpL59h5PW",
        "outputId": "e3cda456-23a5-4822-b132-e7d997f6133d"
      },
      "source": [
        "# So read in as text \n",
        "txt_data = ''\n",
        "encoding = 'utf-8'\n",
        "for f in files_list:\n",
        "  fpath = os.path.join(dpath, f)\n",
        "  print(\"reading {0} {1}\".format(f, now()))\n",
        "  with open(fpath, mode='r',encoding=encoding) as fp:\n",
        "    new_data = fp.read()\n",
        "  print(\"len {0} {1}\".format(len(new_data), datetime.datetime.now()))\n",
        "  txt_data += new_data\n",
        "\n",
        "print(\"len of txt data {0}\".format(len(txt_data)))\n",
        "print(now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading 16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json 2020-12-06 19:12:12.295694\n",
            "len 561492 2020-12-06 19:12:12.338889\n",
            "reading 16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json 2020-12-06 19:12:12.339057\n",
            "len 522019220 2020-12-06 19:12:18.863726\n",
            "len of txt data 522580712\n",
            "2020-12-06 19:12:19.613879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFBKZUYmAqM5",
        "outputId": "6590336b-fbd6-4d27-e70a-b6a6a4db02ff"
      },
      "source": [
        "# split into lines\n",
        "# eval each line\n",
        "# WARNING: should not do this with untrusted files\n",
        "txt_lines = txt_data.split('\\n')\n",
        "print(\"{0} lines\".format(len(txt_lines)))\n",
        "dlist = []\n",
        "for i, l in enumerate(txt_lines):\n",
        "  try:\n",
        "    d = eval(l) \n",
        "    dlist.append(d)\n",
        "  except Exception as e:\n",
        "    max_out = 30\n",
        "    msg = \"line{0}, {1}...{2} \\n{3}\".format(i,l[:20], l[-20:], e)\n",
        "    warnings.warn(msg)\n",
        "print(\"len dlist {0}\".format(len(dlist)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94402 lines\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "line:14  cat:<UserWarning>  msg:line103, {\"organizations\": []...highlightTitle\": \"\"} \n",
            "invalid syntax (<string>, line 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len dlist 94401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygdUHiJgDZ5S",
        "outputId": "381b0406-757d-473c-ecb7-a2cb3e0fe171"
      },
      "source": [
        "#extract text and title\n",
        "dataset = [d['text'] for d in dlist]\n",
        "target = [d['title'] for d in dlist]\n",
        "# The length of the list dataset and target will be 190138.\n",
        "print(\"dataset len= {0}, target len= {1}\\n\".format(len(dataset), len(target)))\n",
        "names = (\"dataset\", \"target\")\n",
        "n = 2\n",
        "for name in names:\n",
        "  print(\"first {0} from {1}\".format(n,name))\n",
        "  items = eval(name)[:n]\n",
        "  maxlen = 30\n",
        "  for i, item in enumerate(items):\n",
        "    suffix = \"...\" if len(item) > maxlen else \"\"\n",
        "    print(\"{0}: {1}{2}\".format(i, item[:maxlen], suffix))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset len= 94401, target len= 94401\n",
            "\n",
            "first 2 from dataset\n",
            "0: Dublin, The “Swine Healthcare ...\n",
            "1: FDA launches app for health ca...\n",
            "\n",
            "first 2 from target\n",
            "0: Global Swine Healthcare Market...\n",
            "1: FDA launches app for health ca...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyuzGAmEiljm"
      },
      "source": [
        "contraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALIyEgLuiopU",
        "outputId": "c85c075b-d895-44ab-e630-f740e1650003"
      },
      "source": [
        "# took a litte trial and error to get that\n",
        "# have to call nltk.download() first\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))  \n",
        "print(type(stop_words))\n",
        "print(list(stop_words)[:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<class 'set'>\n",
            "['mightn', 'can', 'were', 'not']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DU_hVJy95xU",
        "outputId": "cc22dc88-ff54-49f3-8b06-6e8dbca6699c"
      },
      "source": [
        "# my preprocess funtion\n",
        "def preprocess(text, contraction_map= None, stop_words = None):\n",
        "  if not contraction_map:\n",
        "    contraction_map = {}\n",
        "  if not stop_words:\n",
        "    stop_words = []\n",
        "  if not isinstance(stop_words, set):\n",
        "    stop_words = set(stop_words)\n",
        "\n",
        "  # Split the text using Python split() function\n",
        "  toks = text.split()\n",
        "  # Apply the contraction hashmap on all the words of the text\n",
        "  toks = [contraction_map[x] if x in contraction_map.keys() else x for x in toks]  \n",
        "  # Remove the stopwords that are in English\n",
        "  toks = [x for x in toks if x not in stop_words]\n",
        "\n",
        "  # now rejoin to apply other transforms on text string\n",
        "  res = ' '.join(toks)\n",
        "  # Convert text to lowercase\n",
        "  res = res.lower()    \n",
        "  # Remove 's. For example yours becomes your\n",
        "  res = res.replace(\"'s\",'') # convert your's -> your\n",
        "  # Use regular expression to remove parentheses outside a word. For example (word) becomes word\n",
        "  res = re.sub(\"\\(|\\)\", \"\", res) \n",
        "  # Use regular expression to remove punctuations\n",
        "  res = re.sub(r'[^a-zA-Z0-9. ]','',res)\n",
        "  # Use regular expression to add a space character before and after the full stop. For example . becomes .\n",
        "  res = re.sub(\"\\.\", \" . \", res)\n",
        "  return res\n",
        "test = \"The boys can't .(go) there!.\"\n",
        "test_out = preprocess(test, \n",
        "           contraction_map=contraction_map, \n",
        "           stop_words=stop_words)\n",
        "print(\"{0}\\n{1}\".format(test, test_out))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The boys can't .(go) there!.\n",
            "the boys cannot  . go there . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvwgRrUZ6NKN",
        "outputId": "8f70174b-a05a-4ec5-bd10-3c6ae0abb5b0"
      },
      "source": [
        "# suggested preprocess function\n",
        "def preprocess_ref(text, contraction_map, stop_words):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_map:\n",
        "            text[i] = contraction_map[word]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
        "    text = re.sub(r'\\.',' . ',text)\n",
        "    return text\n",
        "test = \"The boys can't .(go) there!.\"\n",
        "test_out = preprocess(test, \n",
        "           contraction_map=contraction_map, \n",
        "           stop_words=stop_words)\n",
        "print(\"{0}\\n{1}\".format(test, test_out))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The boys can't .(go) there!.\n",
            "the boys cannot  . go there . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS9ywQuj3WYU",
        "outputId": "0c85a260-cc1b-4280-f019-66c1623b69bb"
      },
      "source": [
        "%time X = [preprocess(line, contraction_map=contraction_map, stop_words=stop_words) for line in dataset]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22 s, sys: 125 ms, total: 22.1 s\n",
            "Wall time: 22.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DTek3AM6YzT",
        "outputId": "d65de0e5-ceff-42b8-f8f7-f470eeee9c4f"
      },
      "source": [
        "%time X_ref = [preprocess_ref(line, contraction_map=contraction_map, stop_words=stop_words) for line in dataset]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24.1 s, sys: 133 ms, total: 24.2 s\n",
            "Wall time: 24.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U97VFu6Qm6yY",
        "outputId": "4a4398e6-27ad-432e-de25-8f79e12e22f9"
      },
      "source": [
        "%time Y = [preprocess(line, contraction_map=contraction_map, stop_words=stop_words) for line in target]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 829 ms, sys: 7.85 ms, total: 836 ms\n",
            "Wall time: 836 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sW2ra767bV5",
        "outputId": "fc6780b8-df10-43ee-985c-dbab5b90185a"
      },
      "source": [
        "%time Y_ref = [preprocess_ref(line, contraction_map=contraction_map, stop_words=stop_words) for line in target]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 855 ms, sys: 32.8 ms, total: 888 ms\n",
            "Wall time: 887 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9exip0tP8PA1",
        "outputId": "42e1f05d-b351-4628-d4de-e7b3f811f831"
      },
      "source": [
        "# At this point, the length of X and Y should both be 190138.\n",
        "print(\"len X= {0}, len Y= {1}\".format(len(X), len(Y)))\n",
        "print(\"len X_ref= {0}, len Y_ref= {1}\".format(len(X_ref), len(Y_ref)))\n",
        "#assert(len(x) == 190138)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len X= 94401, len Y= 94401\n",
            "len X_ref= 94401, len Y_ref= 94401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyldRzFxE6go",
        "outputId": "48a55841-c553-4c38-8f77-d58a97dea022"
      },
      "source": [
        "# How does this relate to the delivarble?\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "max_len_text = 600\n",
        "max_len_target = 30\n",
        "for i in range(len(dataset)):\n",
        "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
        "        short_text.append(dataset[i])\n",
        "        short_summary.append(target[i])\n",
        "\n",
        "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
        "print(\"temp_df shape {0}\".format(temp_df.shape))\n",
        "# len(temp_df) being 130736."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temp_df shape (64892, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "m5wMOl6OE8m2",
        "outputId": "428752bb-e71b-4468-f0f3-52fc7689217d"
      },
      "source": [
        "temp_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FDA launches app for health care professionals...</td>\n",
              "      <td>FDA launches app for health care professionals...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Of all of Regina Yan ’s many traits, an open m...</td>\n",
              "      <td>C-Suite Awards: Regina Yan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  FDA launches app for health care professionals...  FDA launches app for health care professionals...\n",
              "1  Of all of Regina Yan ’s many traits, an open m...                         C-Suite Awards: Regina Yan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3cdEBXyNeWl",
        "outputId": "c63911a9-2fc7-4ff0-a435-a599d81c972b"
      },
      "source": [
        "# remove empty strings from summary and the text column.\n",
        "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
        "df = newdf[newdf['text'].str.strip().astype(bool)]\n",
        "print(\"df shape {0}\".format(df.shape))\n",
        "print(df.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df shape (62357, 2)\n",
            "                                                text                                            summary\n",
            "0  FDA launches app for health care professionals...  FDA launches app for health care professionals...\n",
            "1  Of all of Regina Yan ’s many traits, an open m...                         C-Suite Awards: Regina Yan\n",
            "2  The CURE ID app allows clinicians to share and...  FDA Launches Infectious Disease Crowdsourcing ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HWOJpqNgVj"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViAGaYl7PKP5"
      },
      "source": [
        "df.head(2)\n",
        "X = list(df['text'].values)\n",
        "Y = list(df['summary'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxpzjoD1PbK3"
      },
      "source": [
        "def readData(text, summary):\n",
        "  print(\"Reading lines...\")\n",
        "  input_lang = Lang(text)\n",
        "  output_lang = Lang(summary)\n",
        "  pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
        "  return (input_lang, output_lang, pairs)\n",
        "\n",
        "def prepareData(tlist, slist):\n",
        "  input_lang, output_lang, pairs = readData(tlist, slist)\n",
        "  print(\"Read %s sentence pairs\" % len(pairs))\n",
        "  print(\"Counting words...\")  \n",
        "  [input_lang.addSentence(t) for t in tlist]\n",
        "  [output_lang.addSentence(s) for s in slist] \n",
        "  return input_lang, output_lang, pairs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY_nhTmMT5Oc",
        "outputId": "b69ff2f7-f138-42cd-de39-934d5ffa2701"
      },
      "source": [
        "input, output, pairs = prepareData(X,Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 62357 sentence pairs\n",
            "Counting words...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybE_WimPUeNg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
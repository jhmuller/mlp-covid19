{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhmuller/mlp-covid19/blob/main/mlp_covid19_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Loading and Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changing the indent\n",
    "Enter the snippet below into the browser's javascript console\n",
    "on chrome get to that from \"...\"->more tools->developer tools->console\n",
    "```\n",
    "var cell = Jupyter.notebook.get_selected_cell();\n",
    "var config = cell.config;\n",
    "var patch = {\n",
    "      CodeCell:{\n",
    "        cm_config:{indentUnit:2}\n",
    "      }\n",
    "    }\n",
    "config.update(patch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HNIUVntFYDWe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. imported ['fsspec', 'nltk', 'pandas as pd', 'numpy as np']\n"
     ]
    }
   ],
   "source": [
    "# need to intall this every time on colab, not sure why but I do it\n",
    "import importlib\n",
    "import warnings\n",
    "import pdb\n",
    "import datetime\n",
    "now = datetime.datetime.now\n",
    "def warning_on_one_line(message, category, filename, lineno, file=None, line=None):\n",
    "    return 'line:%s  cat:<%s>  msg:%s\\n'% (lineno, category.__name__, message)  \n",
    "\n",
    "warnings.formatwarning = warning_on_one_line\n",
    "\n",
    "def global_imports(imports=None, verbosity=0):\n",
    "  fails = []\n",
    "  if not imports:\n",
    "    return\n",
    "  if not isinstance(imports, list):\n",
    "    msg = \"imports shoule be a list with elements like\\m\"\n",
    "    msg += \"'foo' or 'foo as bar'\"\n",
    "    warnings.warn(msg)\n",
    "    return\n",
    "  #pdb.set_trace()\n",
    "  for line in imports:\n",
    "    parts = line.split()\n",
    "    if len(parts) not in [1,3] or \\\n",
    "          (len(parts) == 3 and parts[1] != 'as'):\n",
    "      msg = \"<{0}> not a valid import line\\n\"\n",
    "      msg += \"use 'import foo' or 'import foo as bar'\"\n",
    "      warnings.warn(msg)\n",
    "      continue\n",
    "    module_name = parts[0]\n",
    "    asname = parts[0]\n",
    "    if len(parts) == 3:\n",
    "      asname = parts[2]\n",
    "    try:\n",
    "      module = importlib.import_module(module_name)\n",
    "      globals()[asname] = module\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(\"{0} not installed\\Trying to install it\".format(module_name))\n",
    "      try:\n",
    "        !pip install $module_name\n",
    "        module = importlib.import_module(module_name)  \n",
    "        globals()[asname] = module        \n",
    "      except Exception as e2:\n",
    "        print(e2)\n",
    "        fails.append(module_name)\n",
    "  if fails:\n",
    "    msg = \"Not able to import or install\\n{0}\".format(fails)\n",
    "  else:\n",
    "    msg = \"Success. imported {0}\".format(imports)\n",
    "  if verbosity > 0:\n",
    "    print(msg)\n",
    "  return fails\n",
    "    \n",
    "def is_defined(varnames=None, verbosity=0):\n",
    "  if isinstance(varnames, str):\n",
    "    varnames = [varnames]\n",
    "  for v in varnames:\n",
    "    try:\n",
    "      eval(v)\n",
    "      if verbosity > 0:\n",
    "        print(\"'{0}'' is defined\")\n",
    "    except Exception as e:\n",
    "      warnings.warn(\"trying to eval '{0}'\".format(v))\n",
    "      print(e)\n",
    "\n",
    "imports = [ 'fsspec',\n",
    "           'nltk',\n",
    "            'pandas as pd',\n",
    "            'numpy as np']\n",
    "varnames = ['fsspec', 'pd']\n",
    "global_imports(imports, verbosity=1)\n",
    "# check if the imports worked\n",
    "is_defined([x.split()[-1] for x in imports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mpi84TZlHZM4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import unicodedata\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "import fsspec\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import re\n",
    "# Not using torch for this initial part\n",
    "if False:\n",
    "  import torch\n",
    "  import torch.nn as nn\n",
    "  from torch import optim\n",
    "  import torch.nn.functional as F\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CfosqNUxpsT",
    "outputId": "a9c0a374-e3b3-4d6d-953d-29c23c2e36b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3.8.6 of Python\n",
      "__builtin__ <module 'builtins' (built-in)>\n",
      "__builtins__ <module 'builtins' (built-in)>\n",
      "version fsspec of fsspec as 0.8.4 \n",
      "version nltk of nltk as 3.5 \n",
      "version pd of pandas as 1.1.4 \n",
      "version np of numpy as 1.19.4 \n",
      "version re of re as 2.2.1 \n",
      "version json of json as 2.0.9 \n"
     ]
    }
   ],
   "source": [
    "# code to show what versions of modules I'm using\n",
    "import inspect\n",
    "mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items()))\n",
    "vi = sys.version_info\n",
    "print(\"version {0}.{1}.{2} of Python\".format(vi.major, vi.minor, vi.micro))\n",
    "for name, mod in mlist:\n",
    "    mname = name\n",
    "    if name.startswith(\"__\"):\n",
    "      print(name, mod)\n",
    "      continue\n",
    "    if hasattr(mod, \"__version__\"):\n",
    "        mname = name\n",
    "        if hasattr(mod, \"__path__\"):\n",
    "            mname = os.path.split(mod.__path__[0])[1]\n",
    "        print(\"version {1} of {0} as {2} \".format(mname, name, mod.__version__))\n",
    "    elif hasattr(mod, \"__file__\") and \"site-packages\" in mod.__file__:\n",
    "        print(\"No __version__ for {0} as {1}\".format(mname, name))\n",
    "del mod\n",
    "del name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  # to mount google drive for the input files\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=False)  \n",
    "  dpath = \"/content/gdrive/My Drive/data/mlp_covid19/16119_db21c91a1ab47385bb13773ed8238c31\"  \n",
    "else:\n",
    "  print('Not running on CoLab')\n",
    "  dpath = \"./data/16119_db21c91a1ab47385bb13773ed8238c31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqcsZjItfGB3",
    "outputId": "9116b4fe-bbe5-4442-c354-5e64bf6e1a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json', '16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json', '16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000002.json', '16119_webhose_2020_02_db21c91a1ab47385bb13773ed8238c31_0000001.json']\n"
     ]
    }
   ],
   "source": [
    "# make a list of all the file paths in the data directory\n",
    "jfiles = os.listdir(dpath)\n",
    "jpaths = [os.path.join(dpath, jf) for jf in jfiles]\n",
    "print(jfiles[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rzVXqfZ1fcJJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json 0.56  MB  0.001 GB\n",
      "16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json 524.38  MB  0.524 GB\n"
     ]
    }
   ],
   "source": [
    "# only using 2 files\n",
    "# 16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json \n",
    "# 16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json.\n",
    "files_list = [\"16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json\",\n",
    "      \"16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json\"]\n",
    "#check that these are indeed in the directory\n",
    "all_files = set(os.listdir(dpath))\n",
    "for f in files_list:\n",
    "    if f not in all_files:\n",
    "        warnings.warn(\"{0} not found\".format(f))\n",
    "        continue\n",
    "    fpath = os.path.join(dpath, f)\n",
    "    if not os.path.isfile(fpath):\n",
    "        warnings.warn(\"{0} found but not a file\".format(f))\n",
    "    fbytes = os.path.getsize(fpath)\n",
    "    print(\"{0} {1}  MB  {2} GB\".format(f, np.round(fbytes/10**6, 2),\n",
    "                                       np.round(fbytes/10**9, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIy8kOgDfU_R",
    "outputId": "c78d2938-02c3-471c-bf76-03af12e917ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 2 column 1 (char 3898)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line:13  cat:<UserWarning>  msg:<class 'json.decoder.JSONDecodeError'>\n"
     ]
    }
   ],
   "source": [
    "# try reading the data in the first file using the json module\n",
    "# raised error\n",
    "f1 = file_list[0]\n",
    "p1 = os.path.join(dpath, f1)\n",
    "encoding = 'utf-8'\n",
    "try:\n",
    "  with open(p1, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  # sys.exc_info returns (type, value, traceback)\n",
    "  extype, exval, tb = sys.exc_info()\n",
    "  warnings.warn(str(extype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWvvDcg2ki25",
    "outputId": "385080a2-2a3a-4be3-c44e-3509dd208d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trailing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line:12  cat:<UserWarning>  msg:<class 'ValueError'>\n"
     ]
    }
   ],
   "source": [
    "# try reading the data with pandas\n",
    "# raised error\n",
    "f1 = list(files_set)[0]\n",
    "encoding = 'utf-8'\n",
    "p1 = os.path.join(dpath, f1)\n",
    "try:\n",
    "  data = pd.read_json(p1, encoding=encoding)\n",
    "except Exception as e:\n",
    "  print(e)   \n",
    "  # sys.exc_info returns (type, value, traceback)\n",
    "  extype, exval, tb = sys.exc_info()\n",
    "  warnings.warn(str(extype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaivpL59h5PW",
    "outputId": "28f9a726-b657-4eac-c093-a03f39fde9bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json 2020-12-06 09:39:57.478735\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 713: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c985d94adb35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reading {0} {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m       \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len {0} {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtxt_data\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jmull\\anaconda3\\envs\\mlp-covid19\\lib\\encodings\\ascii.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascii_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 713: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# So read in as text \n",
    "txt_data = ''\n",
    "encoding = 'ascii'\n",
    "for fpath in jpaths:\n",
    "  head, tail = os.path.split(fpath)\n",
    "  if tail in files_set:\n",
    "    print(\"reading {0} {1}\".format(tail, datetime.datetime.now()))\n",
    "    with open(jpaths[0], mode='r',encoding=encoding) as fp:\n",
    "      new_data = fp.read()\n",
    "    print(\"len {0} {1}\".format(len(new_data), datetime.datetime.now()))\n",
    "    txt_data += new_data\n",
    "\n",
    "print(\"len of txt data {0}\".format(len(txt_data)))\n",
    "print(now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFBKZUYmAqM5",
    "outputId": "f1ae08fb-bf82-490b-99b3-3e1fbff7de42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 lines\n",
      "len dlist 206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line:14  cat:<UserWarning>  msg:line103, {\"organizations\": []...highlightTitle\": \"\"} \n",
      "invalid syntax (<string>, line 1)\n"
     ]
    }
   ],
   "source": [
    "# split into lines\n",
    "# eval each line\n",
    "# WARNING: should not do this with untrusted files\n",
    "txt_lines = txt_data.split('\\n')\n",
    "print(\"{0} lines\".format(len(txt_lines)))\n",
    "dlist = []\n",
    "for i, l in enumerate(txt_lines):\n",
    "  try:\n",
    "    d = eval(l) \n",
    "    dlist.append(d)\n",
    "  except Exception as e:\n",
    "    max_out = 30\n",
    "    msg = \"line{0}, {1}...{2} \\n{3}\".format(i,l[:20], l[-20:], e)\n",
    "    warnings.warn(msg)\n",
    "print(\"len dlist {0}\".format(len(dlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygdUHiJgDZ5S",
    "outputId": "e5a41492-8f6c-4330-9a4b-221a1938edf5"
   },
   "outputs": [],
   "source": [
    "#extract text and title\n",
    "dataset = [d['text'] for d in dlist]\n",
    "target = [d['title'] for d in dlist]\n",
    "# The length of the list dataset and target will be 190138.\n",
    "print(\"dataset len= {0}, target len= {1}\\n\".format(len(dataset), len(target)))\n",
    "names = (\"dataset\", \"target\")\n",
    "n = 2\n",
    "for name in names:\n",
    "  print(\"first {0} from {1}\".format(n,name))\n",
    "  items = eval(name)[:n]\n",
    "  maxlen = 30\n",
    "  for i, item in enumerate(items):\n",
    "    suffix = \"...\" if len(item) > maxlen else \"\"\n",
    "    print(\"{0}: {1}{2}\".format(i, item[:maxlen], suffix))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyuzGAmEiljm"
   },
   "outputs": [],
   "source": [
    "contraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALIyEgLuiopU",
    "outputId": "fc5e1fcc-2a79-48c6-98e0-8347a1e8d50a"
   },
   "outputs": [],
   "source": [
    "# took a litte trial and error to get that\n",
    "# have to call nltk.download() first\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))  \n",
    "print(type(stop_words))\n",
    "print(list(stop_words)[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DU_hVJy95xU",
    "outputId": "502beb2d-5200-40b7-9831-631180ed229d"
   },
   "outputs": [],
   "source": [
    "# my preprocess funtion\n",
    "def preprocess(text, contraction_map= None, stop_words = None):\n",
    "  if not contraction_map:\n",
    "    contraction_map = {}\n",
    "  if not stop_words:\n",
    "    stop_words = []\n",
    "  if not isinstance(stop_words, set):\n",
    "    stop_words = set(stop_words)\n",
    "\n",
    "  # Split the text using Python split() function\n",
    "  toks = text.split()\n",
    "  # Apply the contraction hashmap on all the words of the text\n",
    "  toks = [contraction_map[x] if x in contraction_map.keys() else x for x in toks]  \n",
    "  # Remove the stopwords that are in English\n",
    "  toks = [x for x in toks if x not in stop_words]\n",
    "\n",
    "  # now rejoin to apply other transforms on text string\n",
    "  res = ' '.join(toks)\n",
    "  # Convert text to lowercase\n",
    "  res = res.lower()    \n",
    "  # Remove 's. For example yours becomes your\n",
    "  res = res.replace(\"'s\",'') # convert your's -> your\n",
    "  # Use regular expression to remove parentheses outside a word. For example (word) becomes word\n",
    "  res = re.sub(\"\\(|\\)\", \"\", res) \n",
    "  # Use regular expression to remove punctuations\n",
    "  res = re.sub(r'[^a-zA-Z0-9. ]','',res)\n",
    "  # Use regular expression to add a space character before and after the full stop. For example . becomes .\n",
    "  res = re.sub(\"\\.\", \" . \", res)\n",
    "  return res\n",
    "test = \"The boys can't .(go) there!.\"\n",
    "test_out = preprocess(test, \n",
    "           contraction_map=contraction_map, \n",
    "           stop_words=stop_words)\n",
    "print(\"{0}\\n{1}\".format(test, test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvwgRrUZ6NKN",
    "outputId": "7d49addb-f3bd-49e0-e21b-9cb782f94013"
   },
   "outputs": [],
   "source": [
    "# suggested preprocess function\n",
    "def preprocess_ref(text, contraction_map, stop_words):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_map:\n",
    "            text[i] = contraction_map[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text\n",
    "test = \"The boys can't .(go) there!.\"\n",
    "test_out = preprocess(test, \n",
    "           contraction_map=contraction_map, \n",
    "           stop_words=stop_words)\n",
    "print(\"{0}\\n{1}\".format(test, test_out))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AS9ywQuj3WYU",
    "outputId": "5bf63b67-0d51-454e-900f-321927dbbbc1"
   },
   "outputs": [],
   "source": [
    "%time X = [preprocess(line, contraction_map=contraction_map, stop_words=stop_words) for line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DTek3AM6YzT",
    "outputId": "96abf311-bd23-4e2b-b3c2-aff678cd7c7c"
   },
   "outputs": [],
   "source": [
    "%time X_ref = [preprocess_ref(line, contraction_map=contraction_map, stop_words=stop_words) for line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U97VFu6Qm6yY",
    "outputId": "ef36bd4c-0916-40e3-b427-f2634f3e5dd4"
   },
   "outputs": [],
   "source": [
    "%time Y = [preprocess(line, contraction_map=contraction_map, stop_words=stop_words) for line in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sW2ra767bV5",
    "outputId": "97d9f862-4bec-4654-8dcf-35de933e6474"
   },
   "outputs": [],
   "source": [
    "%time Y_ref = [preprocess_ref(line, contraction_map=contraction_map, stop_words=stop_words) for line in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9exip0tP8PA1",
    "outputId": "39878640-84b9-48ab-9ea1-184fb5231939"
   },
   "outputs": [],
   "source": [
    "# At this point, the length of X and Y should both be 190138.\n",
    "print(\"len X= {0}, len Y= {1}\".format(len(X), len(Y)))\n",
    "print(\"len X_ref= {0}, len Y_ref= {1}\".format(len(X_ref), len(Y_ref)))\n",
    "#assert(len(x) == 190138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyldRzFxE6go",
    "outputId": "2394f41c-24b6-40e3-8ee7-babd5098dc0a"
   },
   "outputs": [],
   "source": [
    "# How does this relate to the delivarble?\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "max_len_text = 600\n",
    "max_len_target = 30\n",
    "for i in range(len(dataset)):\n",
    "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
    "        short_text.append(dataset[i])\n",
    "        short_summary.append(target[i])\n",
    "\n",
    "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "print(\"temp_df shape {0}\".format(temp_df.shape))\n",
    "# len(temp_df) being 130736."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "m5wMOl6OE8m2",
    "outputId": "acc4a0d7-5d5b-45ff-f790-808031b56035"
   },
   "outputs": [],
   "source": [
    "temp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3cdEBXyNeWl",
    "outputId": "8a3b45ff-3e18-497c-c4db-681379cb3ed2"
   },
   "outputs": [],
   "source": [
    "# remove empty strings from summary and the text column.\n",
    "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
    "df = newdf[newdf['text'].str.strip().astype(bool)]\n",
    "print(\"df shape {0}\".format(df.shape))\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3HWOJpqNgVj"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "ViAGaYl7PKP5",
    "outputId": "7dc5b573-93de-46c4-fd8f-c48e2667aadf"
   },
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "X = list(df['text'].values)\n",
    "Y = list(df['summary'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxpzjoD1PbK3"
   },
   "outputs": [],
   "source": [
    "def readData(text, summary):\n",
    "  print(\"Reading lines...\")\n",
    "  input_lang = Lang(text)\n",
    "  output_lang = Lang(summary)\n",
    "  pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "  return (input_lang, output_lang, pairs)\n",
    "\n",
    "def prepareData(tlist, slist):\n",
    "  input_lang, output_lang, pairs = readData(tlist, slist)\n",
    "  print(\"Read %s sentence pairs\" % len(pairs))\n",
    "  print(\"Counting words...\")  \n",
    "  [input_lang.addSentence(t) for t in tlist]\n",
    "  [output_lang.addSentence(s) for s in slist] \n",
    "  return input_lang, output_lang, pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sY_nhTmMT5Oc",
    "outputId": "932f9d96-d227-4a65-c574-c1a6282061b5"
   },
   "outputs": [],
   "source": [
    "input, output, pairs = prepareData(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybE_WimPUeNg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFRwQdi3IFq4YSbwDO5V4M",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "mlp_covid19-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

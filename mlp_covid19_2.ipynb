{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_covid19-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7EP7Vw51co1QMgmRk7kO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmuller/mlp-covid19/blob/main/mlp_covid19_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlYvPvDW-8F4"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\r\n",
        "import unicodedata\r\n",
        "import string\r\n",
        "import re\r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWzTBU7FR1pR",
        "outputId": "ea86575c-8f04-4ebc-9b82-7e2ab643521d"
      },
      "source": [
        "# need to intall this every time on colab, not sure why but I do it\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import importlib\r\n",
        "import warnings\r\n",
        "import pdb\r\n",
        "import datetime\r\n",
        "now = datetime.datetime.now\r\n",
        "def warning_on_one_line(message, category, filename, lineno, file=None, line=None):\r\n",
        "    return 'line:%s  cat:<%s>  msg:%s\\n'% (lineno, category.__name__, message)  \r\n",
        "\r\n",
        "warnings.formatwarning = warning_on_one_line\r\n",
        "\r\n",
        "def global_imports(imports=None, verbosity=0):\r\n",
        "  fails = []\r\n",
        "  if not imports:\r\n",
        "    return\r\n",
        "  if not isinstance(imports, list):\r\n",
        "    msg = \"imports shoule be a list with elements like\\m\"\r\n",
        "    msg += \"'foo' or 'foo as bar'\"\r\n",
        "    warnings.warn(msg)\r\n",
        "    return\r\n",
        "  #pdb.set_trace()\r\n",
        "  for line in imports:\r\n",
        "    parts = line.split()\r\n",
        "    if len(parts) not in [1,3] or \\\r\n",
        "          (len(parts) == 3 and parts[1] != 'as'):\r\n",
        "      msg = \"<{0}> not a valid import line\\n\"\r\n",
        "      msg += \"use 'import foo' or 'import foo as bar'\"\r\n",
        "      warnings.warn(msg)\r\n",
        "      continue\r\n",
        "    module_name = parts[0]\r\n",
        "    asname = parts[0]\r\n",
        "    if len(parts) == 3:\r\n",
        "      asname = parts[2]\r\n",
        "    try:\r\n",
        "      module = importlib.import_module(module_name)\r\n",
        "      globals()[asname] = module\r\n",
        "    except Exception as e:\r\n",
        "      print(e)\r\n",
        "      print(\"{0} not installed\\Trying to install it\".format(module_name))\r\n",
        "      try:\r\n",
        "        !pip install $module_name\r\n",
        "        module = importlib.import_module(module_name)  \r\n",
        "        globals()[asname] = module        \r\n",
        "      except Exception as e2:\r\n",
        "        print(e2)\r\n",
        "        fails.append(module_name)\r\n",
        "  if fails:\r\n",
        "    msg = \"Not able to import or install\\n{0}\".format(fails)\r\n",
        "  else:\r\n",
        "    msg = \"Success. imported {0}\".format(imports)\r\n",
        "  if verbosity > 0:\r\n",
        "    print(msg)\r\n",
        "  return fails\r\n",
        "    \r\n",
        "def is_defined(varnames=None, verbosity=0):\r\n",
        "  if isinstance(varnames, str):\r\n",
        "    varnames = [varnames]\r\n",
        "  for v in varnames:\r\n",
        "    try:\r\n",
        "      eval(v)\r\n",
        "      if verbosity > 0:\r\n",
        "        print(\"'{0}'' is defined\")\r\n",
        "    except Exception as e:\r\n",
        "      warnings.warn(\"trying to eval '{0}'\".format(v))\r\n",
        "      print(e)\r\n",
        "\r\n",
        "imports = [ 'fsspec',\r\n",
        "           'nltk',\r\n",
        "            'pandas as pd',\r\n",
        "            'numpy as np']\r\n",
        "varnames = ['fsspec', 'pd']\r\n",
        "global_imports(imports, verbosity=1)\r\n",
        "# check if the imports worked\r\n",
        "is_defined([x.split()[-1] for x in imports])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success. imported ['fsspec', 'nltk', 'pandas as pd', 'numpy as np']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oD46Sbxbfwn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBScAAq33gj"
      },
      "source": [
        "# I am *guessing* this is max_len_text from part 1\r\n",
        "max_len_text = 600\r\n",
        "MAX_LENGTH = max_len_text"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftGu34q6frDI",
        "outputId": "cccb620b-8de3-4af0-f206-e8c0ca4e58f5"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\r\n",
        "  print('Running on CoLab')\r\n",
        "  # to mount google drive for the input files\r\n",
        "  from google.colab import drive\r\n",
        "  drive.mount('/content/gdrive', force_remount=False)  \r\n",
        "  dpath = \"/content/gdrive/My Drive/mlp_covid19/data/part1_output\"  \r\n",
        "else:\r\n",
        "  print('Not running on CoLab')\r\n",
        "  dpath = \"./mlp_covid19/data/part1_output\"\r\n",
        "if not os.path.isdir(dpath):\r\n",
        "  msg = \"{0} is not a path\".format(dpath)\r\n",
        "  warnings.warn(msg)\r\n",
        "else:\r\n",
        "  files = os.listdir(dpath)\r\n",
        "  msg = \"{0} has {1} files\".format(dpath, len(files))\r\n",
        "  n2print = 4\r\n",
        "  suffix = \"...\" if len(files) > n2print else \"\"\r\n",
        "  fsizes =[os.path.getsize(os.path.join(dpath, fname)) for fname in files ]\r\n",
        "  for i, f in enumerate(files):\r\n",
        "    msg += \"\\n\\t'{0}' size: {1} MB  {2} GB\".format(f, np.round(fsizes[i]/10**6,2), \r\n",
        "                                             np.round(fsizes[i]/10**9, 3))\r\n",
        "  print(msg)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/mlp_covid19/data/part1_output has 4 files\n",
            "\t'pairs_df.csv' size: 123.95 MB  0.124 GB\n",
            "\t'input_lang.parquet' size: 141.22 MB  0.141 GB\n",
            "\t'output_lang.parquet' size: 6.41 MB  0.006 GB\n",
            "\t'pairs.parquet' size: 124.92 MB  0.125 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3_JOii7j-8r"
      },
      "source": [
        "SOS_token = 0\r\n",
        "EOS_token = 1\r\n",
        "MAX_LENGTH = 20\r\n",
        "\r\n",
        "SOS_token = 0\r\n",
        "EOS_token = 1\r\n",
        "\r\n",
        "class Lang:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "        self.word2index = {}\r\n",
        "        self.word2count = {}\r\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\r\n",
        "        self.n_words = 2  # Count SOS and EOS\r\n",
        "\r\n",
        "    def addSentence(self, sentence):\r\n",
        "        for word in sentence.split(' '):\r\n",
        "            self.addWord(word)\r\n",
        "\r\n",
        "    def addWord(self, word):\r\n",
        "        if word not in self.word2index:\r\n",
        "            self.word2index[word] = self.n_words\r\n",
        "            self.word2count[word] = 1\r\n",
        "            self.index2word[self.n_words] = word\r\n",
        "            self.n_words += 1\r\n",
        "        else:\r\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RxQMSpue6v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcec33b5-e781-4d52-fc93-ef785adc975a"
      },
      "source": [
        "# read data in from part 1\r\n",
        "import pickle\r\n",
        "parq_files = [fname for fname in os.listdir(dpath) if fname.endswith(\"parquet\")]\r\n",
        "for pfile in parq_files:\r\n",
        "  obj_name = pfile.split(\".\")[0]\r\n",
        "  fpath = os.path.join(dpath, pfile)\r\n",
        "  if not os.path.isfile(fpath):\r\n",
        "    warnings.warn(\"{0} is not a filepath\".format(fpath))\r\n",
        "    continue\r\n",
        "  try:\r\n",
        "    print(\"trying to read {0} from {1} {2}\".format(pfile, dpath, \r\n",
        "                                                 datetime.datetime.now()))    \r\n",
        "    with open(fpath, mode='rb') as file:\r\n",
        "      obj = pickle.load(file)\r\n",
        "      print(\"Got it\")\r\n",
        "      globals()[obj_name] = obj\r\n",
        "    print(\"\\nGot it, {0} is a {1}\".format(obj_name, type(eval(obj_name))))      \r\n",
        "  except Exception as e:\r\n",
        "    warnings.warn(\"*Read Failed*\")\r\n",
        "    warnings.warn(str(e))    "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trying to read input_lang.parquet from /content/gdrive/My Drive/mlp_covid19/data/part1_output 2020-12-12 20:14:28.429750\n",
            "Got it\n",
            "\n",
            "Got it, input_lang is a <class '__main__.Lang'>\n",
            "trying to read output_lang.parquet from /content/gdrive/My Drive/mlp_covid19/data/part1_output 2020-12-12 20:14:29.564535\n",
            "Got it\n",
            "\n",
            "Got it, output_lang is a <class '__main__.Lang'>\n",
            "trying to read pairs.parquet from /content/gdrive/My Drive/mlp_covid19/data/part1_output 2020-12-12 20:14:29.627900\n",
            "Got it\n",
            "\n",
            "Got it, pairs is a <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyySf9hswk6I"
      },
      "source": [
        "'''\r\n",
        "Reusable set of functions to convert a tuple of strings (pair) to tensors\r\n",
        "Reference: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\r\n",
        "'''\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "def indexesFromSentence(lang, sentence):\r\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\r\n",
        "\r\n",
        "\r\n",
        "def tensorFromSentence(lang, sentence):\r\n",
        "  indexes = indexesFromSentence(lang, sentence)\r\n",
        "  indexes.append(EOS_token)\r\n",
        "  return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\r\n",
        "\r\n",
        "\r\n",
        "def tensorsFromPair(pair):\r\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\r\n",
        "  target_tensor = tensorFromSentence(output_lang, pair[1])\r\n",
        "  return (input_tensor, target_tensor)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b7Aav5QQkQG"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_size):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\r\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\r\n",
        "\r\n",
        "  def init_hidden(self):\r\n",
        "    self.hidden = torch.zeros(1, 1, self.hidden_size, device=device)\r\n",
        "\r\n",
        "  def forward(self, input, hidden):\r\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\r\n",
        "    output = embedded\r\n",
        "    output, hidden = self.gru(output, hidden)\r\n",
        "    return output, hidden"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it3SJ2DvmhOB"
      },
      "source": [
        "class AttnDecoder(nn.Module):\r\n",
        "  def __init__(self, hidden_size, output_size, dropout=0.1):\r\n",
        "    super(AttnDecoder, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "\r\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\r\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\r\n",
        "    self.out = nn.Linear(hidden_size, output_size)\r\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "  def forward(self, input, hidden):\r\n",
        "    output = self.embedding(input).view(1, 1, -1)\r\n",
        "    output = F.relu(output)\r\n",
        "    output, hidden = self.gru(output, hidden)\r\n",
        "    output = self.softmax(self.out(output[0]))\r\n",
        "    return output, hidden\r\n",
        "\r\n",
        "  def init_hidden(self):\r\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNuL6gyuAfyX"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "def asMinutes(s):\r\n",
        "  m = math.floor(s / 60)\r\n",
        "  s -= m * 60\r\n",
        "  return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "\r\n",
        "def timeSince(since, percent):\r\n",
        "  now = time.time()\r\n",
        "  s = now - since\r\n",
        "  es = s / (percent)\r\n",
        "  rs = es - s\r\n",
        "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iacrcUt8v5jW"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, \r\n",
        "               learning_rate=0.01, verbosity = 0):\r\n",
        "      \r\n",
        "  start_dt = time.time()\r\n",
        "  plot_losses = []\r\n",
        "  print_loss_total = 0  # Reset every print_every\r\n",
        "  plot_loss_total = 0  # Reset every plot_every  \r\n",
        "\r\n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\r\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\r\n",
        "  training_pairs = [tensorsFromPair(random.choice(pairs))for i in range(n_iters)]\r\n",
        "  criterion = nn.NLLLoss()\r\n",
        "\r\n",
        "  if verbosity > 0:\r\n",
        "    print(\"Start iter {0}\".format(start_dt))\r\n",
        "  losses = []\r\n",
        "  for iter in range(1, n_iters + 1):\r\n",
        "    if iter% 1000 == 0:\r\n",
        "        print(iter,\"/\",n_iters + 1)\r\n",
        "    training_pair = training_pairs[iter - 1]\r\n",
        "    input_tensor = training_pair[0]\r\n",
        "    target_tensor = training_pair[1]\r\n",
        "\r\n",
        "    loss = train(input_tensor=input_tensor, target_tensor=target_tensor, \r\n",
        "                 encoder=encoder, decoder=decoder, \r\n",
        "                 encoder_optimizer=encoder_optimizer, \r\n",
        "                 decoder_optimizer=decoder_optimizer, \r\n",
        "                 criterion=criterion,\r\n",
        "                 verbosity=verbosity) \r\n",
        "    if verbosity > 0:  \r\n",
        "      if iter % print_every == 0:\r\n",
        "        print_loss_avg = print_loss_total / print_every\r\n",
        "        print_loss_total = 0\r\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\r\n",
        "                                      iter, iter / n_iters * 100, \r\n",
        "                                     print_loss_avg))\r\n",
        "\r\n",
        "    if iter % plot_every == 0:\r\n",
        "      plot_loss_avg = plot_loss_total / plot_every\r\n",
        "      plot_losses.append(plot_loss_avg)\r\n",
        "      plot_loss_total = 0\r\n",
        "  \r\n",
        "  showPlot(plot_losses)      \r\n",
        "\r\n",
        "def  train(input_tensor, target_tensor, encoder, decoder, \r\n",
        "           encoder_optimizer, decoder_optimizer, \r\n",
        "           criterion, max_length=MAX_LENGTH, \r\n",
        "           verbosity=0):\r\n",
        "  encoder_hidden = encoder.init_hidden()\r\n",
        "\r\n",
        "  encoder_optimizer.zero_grad()\r\n",
        "  decoder_optimizer.zero_grad()\r\n",
        "\r\n",
        "  input_length = input_tensor.size(0)\r\n",
        "  target_length = target_tensor.size(0)\r\n",
        "\r\n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\r\n",
        "  loss = 0\r\n",
        "  for i in range(input_length):\r\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\r\n",
        "    encoder_outputs[i] = encoder_output[0, 0]\r\n",
        "  \r\n",
        "  decoder_input = torch.tensor([[SOS_token]], device=device)\r\n",
        "  \r\n",
        "  decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "  teacher_forcing_ratio = 0.5  \r\n",
        "  use_teacher_forcing = True if random.random < techer_forcing_raio else False\r\n",
        "  if use_teacher_forcing:\r\n",
        "    # Teacher forcing: Feed the target as the next input\r\n",
        "    for di in range(target_length):\r\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "           decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "      loss += criterion(decoder_output, target_tensor[di])\r\n",
        "      decoder_input = target_tensor[di]  # Teacher forcing\r\n",
        "\r\n",
        "  else:\r\n",
        "    # Without teacher forcing: use its own predictions as the next input\r\n",
        "    for di in range(target_length):\r\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "          decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "      topv, topi = decoder_output.topk(1)\r\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\r\n",
        "\r\n",
        "      loss += criterion(decoder_output, target_tensor[di])\r\n",
        "      if decoder_input.item() == EOS_token:\r\n",
        "        break\r\n",
        "\r\n",
        "  loss.backward()\r\n",
        "\r\n",
        "  encoder_optimizer.step()\r\n",
        "  decoder_optimizer.step()  \r\n",
        "\r\n",
        "  return loss.item / target_length\r\n",
        "\r\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpfHROHFDO5r"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH,\r\n",
        "             verbosity=0):\r\n",
        "  with torch.no_grad():\r\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence)\r\n",
        "    input_length = input_tensor.size()[0]\r\n",
        "    encoder_hidden = encoder.init_hidden()\r\n",
        "\r\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\r\n",
        "    if verbosity > 0:\r\n",
        "      print(\"encoder outputs.sape {0}\".format(encoder_outputs.shape))\r\n",
        "      print(\"input_length= {0} initialing encoder input\".format(input_length))\r\n",
        "    for ei in range(input_length):\r\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei],\r\n",
        "                                                  encoder_hidden)\r\n",
        "        if verbosity > 0:\r\n",
        "          print(\"{0} type \".format(type(encoder_output)))\r\n",
        "        encoder_outputs[ei] += encoder_output[0, 0]\r\n",
        "\r\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\r\n",
        "\r\n",
        "    decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "    decoded_words = []\r\n",
        "    decoder_attentions = torch.zeros(max_length, max_length)\r\n",
        "\r\n",
        "    for di in range(max_length):\r\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "          decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "      decoder_attentions[di] = decoder_attention.data\r\n",
        "      topv, topi = decoder_output.data.topk(1)\r\n",
        "      if topi.item() == EOS_token:\r\n",
        "        decoded_words.append('<EOS>')\r\n",
        "        break\r\n",
        "      else:\r\n",
        "        decoded_words.append(output_lang.index2word[topi.item()])\r\n",
        "\r\n",
        "        decoder_input = topi.squeeze().detach()\r\n",
        "\r\n",
        "    return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpQnT9WcDhIt"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\r\n",
        "  for i in range(n):\r\n",
        "    pair = random.choice(pairs)\r\n",
        "    print('>', pair[0])\r\n",
        "    print('=', pair[1])\r\n",
        "    output_words, attentions = evaluate(encoder, decoder, pair[0])\r\n",
        "    output_sentence = ' '.join(output_words)\r\n",
        "    print('<', output_sentence)\r\n",
        "    print('')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "amkt6QeabPdd",
        "outputId": "75ab355a-a2f8-415e-8ea0-9834377b2585"
      },
      "source": [
        "hidden_size = 300\r\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\r\n",
        "decoder = AttnDecoder(hidden_size, output_lang.n_words, dropout=0.1).to(device)\r\n",
        "\r\n",
        "trainIters(encoder=encoder, decoder=decoder, n_iters=1000, verbosity=2)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start iter 1607804110.54732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-965e993ca2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-8cb023e9888a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate, verbosity)\u001b[0m\n\u001b[1;32m     27\u001b[0m                  \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                  \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                  verbosity=verbosity) \n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbosity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-8cb023e9888a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, verbosity)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for dimension 0 with size 20"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRgagpz1cwl6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}